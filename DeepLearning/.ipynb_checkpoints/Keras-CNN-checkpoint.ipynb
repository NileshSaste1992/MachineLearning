{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Keras\n",
    "Let's use Keras on the MNIST data set again, this time using a Convolutional Neural Network that's better suited for image processing. CNN's are less sensitive to where in the image the pattern is that we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Dense,Dropout,MaxPool2D,Flatten\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to shape the data differently then before. Since we're treating the data as 2D images of 28x28 pixels instead of a flattened stream of 784 pixels, we need to shape it accordingly. Depending on the data format Keras is set up for, this may be 1x28x28 or 28x28x1 (the \"1\" indicates a single color channel, as this is just grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_train_data,mnist_train_label),(mnist_test_data,mnist_test_label) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format == \"channels_first\":\n",
    "    train_data = mnist_train_data.reshape(mnist_train_data.shape[0],1,28,28)\n",
    "    test_data = mnist_test_data.reshape(mnist_test_data.shape[0],1,28,28)\n",
    "    input_shape = (1,28,28)\n",
    "else:\n",
    "    train_data = mnist_train_data.reshape(mnist_train_data.shape[0],28,28,1)\n",
    "    test_data = mnist_test_data.reshape(mnist_test_data.shape[0],28,28,1)\n",
    "    input_shape = (28,28,1)\n",
    "    \n",
    "train_data,test_data = train_data.astype('float32'),test_data.astype('float32')\n",
    "train_data,test_data = train_data/255,test_data/255\n",
    "\n",
    "#As before we need to convert our train and test labels to be categorical in one-hot format:\n",
    "train_label = tf.keras.utils.to_categorical(mnist_train_label,10)\n",
    "test_label = tf.keras.utils.to_categorical(mnist_test_label,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEvdJREFUeJzt3X/wVXWdx/HnS1RqATeRL2qKfAvR1Zoi9yu1K6VO6ZI6qbObQeqgm9GOZjFrzirbiJs42e6SNpEYCEmTWM6UguSULJouOpnfHAcwJlOXFEX4Av4AtVz0vX+c83WvX+4993J/fO+Fz+sxc+d7Oe9z7nnfw33dc+4599yjiMDM0rNPuxsws/Zw+M0S5fCbJcrhN0uUw2+WKIffLFEOfxtJukXS7Hb30W6SfiXpohrHXS/pU3XOp+5p90YOf4n8xbFJ0rCSYRdJ+lUb22oaSedIekjSawOfk6SjJC2V1Cdpm6RfSjq6pD5F0u8lvSxps6TFkg4oM4/xkv4k6UeD8JRaRtIMSU9LekXS85Kul7Rvu/tqJod/V/sCX213E7tL0pAaRtsG3ABcV6b2HmAZcDRwMPAbYGlJ/UHghIj4S+D9ZMup3FbL94BHau+8Y90FHBcRBwAfBD4MfKW9LTWXw7+r/wC+Juk9AwuSuiVF6RqgdJNV0gWSHszXEi/la46/zYc/m68xpw142FGSVkjaLul+SWNLHvuv8tq2fK17TkntFknzJN0t6VXg5GpPLCL+KyJuB54vU/tNRCyMiG0R8b/A9cDRkg7K689GxJaSSd4EjhywfKYALwErq/VSiaRxku6VtFXSFkm3lvm/OF7S7yS9KOkHkt5VMv0Zkh7Ll/9Dkj5UTx8R8VREvNT/sMBbDHi+ezqHf1e9wK+Ar9U5/UeB1cBBwBLgx8DxZC+c84C5koaXjH8ucA0wCngMuBUg/+ixIn+M0cBU4EZJHyiZ9vPAtcAIYJWkz0taXWffA30CeCEitvYPkDRJ0svAduDvybYi+msHAN8ALmtwvgK+CbwXOAYYA1w9YJxzgb8DxgFHAV/PezgOWAR8iWz5fx9YJmnoLjPJnstLA4cPGOfzkl4BtpCt+b9f97PqQA5/eVcBl0rqqmPa/4mIH0TEm8BPyF6834iIP0fEPcAbvHMN8vOIeCAi/gz8K/A3ksYAZwDr88faGRGPAj8F/qFk2qUR8WBEvBURf4qIJRFR15qulKTDyTbf/7l0eESsyjf7DyfbQlpfUr4GWBgRzzYy74h4MiJW5MurD/g2cOKA0ebmWyLbyN78pubDvwh8PyIejog3I2Ix8GfgY2Xmsyoidtm6GzDOknyz/yjgJmBTI8+t0zj8ZUTEWmA5cEUdk5e+QF7PH2/gsNI1/9thiYgdZJ/L3wuMBT6ab76+lK+lzgUOKTdts+RvePcAN0bEbeXGiYjngF+QbdUgaQLwKbKPCo3Of7SkH0t6Ll/r/ohsq6hU6fP+I9nygmyZXTZgmY0pqdclIv4APA7c2MjjdJq9au9lk80CHgXmlAx7Nf/7F8Ar+f3SMNZjTP+d/OPASLLP5M8C90fEKQXTNvWUTEkHkgV/WURcW2X0fck2uwFOArqBZyRB9uY2RNKxEXHcbrbxTbLn9aGI2CrpLGDugHHGlNw/gv/fh/EscG0Nvdej9PnuFbzmryAiniTbbP9KybA+4DngPElDJP0jjb8gTss/f+5Ptun8cL7pvBw4StL5kvbLb8dLOqbeGeU9v4vshbyPpHdJ2i+vHQD8EngwInbZ4pF0rqQjlBlLtrndv2NvPtlymJDfbgJ+Tva5vHRHaXcNbY4AdgAvSToMuLzMOJdIOlzSSGAm2f8TwALgnyR9NO9zmKTTJY2oYb4Dn+9Fkkbn948FrqSBHZmdyOEv9g1g2IBhXyR7QW4FPgA81OA8lpBtZWwD/pps056I2A6cCkwhW7O9AHwL2GXnVb88oI8XzOt8so8d84CP5/cX5LWzyXZMXihpR8ntiLx+LNlz3UF22O/3ZMuCiHgtIl7ov+Xj/Cl/s4RsTf1HsjfOav4NOA54mewN5GdlxllCtoXydH6bnffRm/c0F3gReBK4oNxMJH1c0o6CPk4A1uRHUu7ObzNr6H+PIf+Yh7WapK8DfRGxV+0t39M5/GaJ8ma/WaIcfrNEOfxmiRrU4/yjRo2K7u7uwZylWVLWr1/Pli1bVMu4DYVf0mTgO8AQ4OaIKHe22Nu6u7vp7e1tZJZmVqCnp6fmceve7Fd2Cun3gE+THQOemn8Zwsz2AI185p8IPBkRT0fEG2Tf8z6zOW2ZWas1Ev7DeOcJFhvyYe8gabqkXkm9fX19A8tm1iaNhL/cToVdvjEUEfMjoicierq66jlD1sxaoZHwb+CdZ1cdTplfiDGzztRI+B8Bxkt6X35G2hSy34Azsz1A3Yf6ImKnpC+TnQY6BFgUEUVnlJlZB2noOH9E9J/qaGZ7GH+91yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJcvjNEuVLdO/lLr744sL6vHnzCutXXXVVYf28884rrI8fP76wbu3jNb9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlMNvligf50+cVHw159mzZxfWb7/99sL6ggULKtaOP/74wmmHDh1aWLfGeM1vliiH3yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyXKx/n3chdeeGFD0y9cuLCw/sQTTxTWTzzxxIq1devWFU571FFHFdatMQ2FX9J6YDvwJrAzInqa0ZSZtV4z1vwnR8SWJjyOmQ0if+Y3S1Sj4Q/gHkm/lTS93AiSpkvqldTb19fX4OzMrFkaDf8JEXEc8GngEkmfGDhCRMyPiJ6I6Onq6mpwdmbWLA2FPyKez/9uBu4AJjajKTNrvbrDL2mYpBH994FTgbXNaszMWquRvf0HA3fk54PvCyyJiF80pStrmmrnzFerDx8+vLA+Z86c3e6p3+WXX15YX7p0ad2PbdXVHf6IeBr4cBN7MbNB5EN9Zoly+M0S5fCbJcrhN0uUw2+WKJ/Sa4Wuueaawvq73/3uwnrRT3/fe++9hdPed999hfWTTz65sG7FvOY3S5TDb5Yoh98sUQ6/WaIcfrNEOfxmiXL4zRLl4/xWqNplsi+44ILCetFx/tdee61w2tdff72wbo3xmt8sUQ6/WaIcfrNEOfxmiXL4zRLl8JslyuE3S5SP81uhG264obC+aNGiuh/7mGOOKawfffTRdT+2Vec1v1miHH6zRDn8Zoly+M0S5fCbJcrhN0uUw2+WKB/n3wusWLGiYm3u3LmF095///2F9Wrn1O/cubOwXmTcuHEN1a0xVdf8khZJ2ixpbcmwkZJWSPpD/vfA1rZpZs1Wy2b/LcDkAcOuAFZGxHhgZf5vM9uDVA1/RDwAbBsw+ExgcX5/MXBWk/sysxard4ffwRGxESD/O7rSiJKmS+qV1NvX11fn7Mys2Vq+tz8i5kdET0T0dHV1tXp2ZlajesO/SdKhAPnfzc1rycwGQ73hXwZMy+9PA5Y2px0zGyxVj/NLug04CRglaQMwC7gOuF3SF4BngM+2skkrVvTb+KtWrSqcNiIK65IK6yNGjCisL1++vGLtoIMOKpzWWqtq+CNiaoXSJ5vci5kNIn+91yxRDr9Zohx+s0Q5/GaJcvjNEuVTeq0hb7zxRmF969atFWuTJk1qdju2G7zmN0uUw2+WKIffLFEOv1miHH6zRDn8Zoly+M0S5eP8e4FqP79d5OKLLy6sv/DCC4X1O++8s7B+9tlnV6ydccYZhdMuW7assG6N8ZrfLFEOv1miHH6zRDn8Zoly+M0S5fCbJcrhN0uUj/Mn7sYbbyysv/rqq4X1KVOmFNbvvvvuirUXX3yxcNpt2wZeIvKdRo4cWVi3Yl7zmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJ8nF+KzRs2LDC+owZMwrrRcf5H3roocJpf/3rXxfWTzvttMK6Fau65pe0SNJmSWtLhl0t6TlJj+U3/y+Y7WFq2ey/BZhcZvj1ETEhv1V+ezezjlQ1/BHxAFD8PUsz2+M0ssPvy5JW5x8LDqw0kqTpknol9fb19TUwOzNrpnrDPw8YB0wANgJzKo0YEfMjoicierq6uuqcnZk1W13hj4hNEfFmRLwFLAAmNrctM2u1usIv6dCSf54NrK00rpl1pqrH+SXdBpwEjJK0AZgFnCRpAhDAeuBLLezROlhPT0+7W7A6VQ1/REwtM3hhC3oxs0Hkr/eaJcrhN0uUw2+WKIffLFEOv1mifErvIHj99dcL69VOi50zp+IXKAEYPnz4bvfULGvWrGnbvK0xXvObJcrhN0uUw2+WKIffLFEOv1miHH6zRDn8Zonycf4mqHYc/8orryys33zzzYX1Qw45pLA+c+bMirWhQ4cWTtuom266qe5pJ04s/g0Yny7cWl7zmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJ8nH+Jli5cmVh/bvf/W5Djz979uzC+imnnFKxNmnSpMJpi74jUIvVq1fXPe1FF11UWB89enTdj23Vec1vliiH3yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyWqlkt0jwF+CBwCvAXMj4jvSBoJ/AToJrtM9zkR8WLrWu1ckydPLqw/9dRThfXPfOYzhfXHH3+8sH766adXrO2zT/H7+8svv1xYl1RYb8Spp57asse26mpZ8+8ELouIY4CPAZdIOha4AlgZEeOBlfm/zWwPUTX8EbExIh7N728H1gGHAWcCi/PRFgNntapJM2u+3frML6kb+AjwMHBwRGyE7A0C8HcxzfYgNYdf0nDgp8CMiHhlN6abLqlXUm9fX189PZpZC9QUfkn7kQX/1oj4WT54k6RD8/qhwOZy00bE/IjoiYierq6uZvRsZk1QNfzKdvcuBNZFxLdLSsuAafn9acDS5rdnZq1Syym9JwDnA2skPZYPmwlcB9wu6QvAM8BnW9Ni59t33+LF2N3dXVi/6667Cut33HFHYX3WrFkVa6+8UvMntLocccQRhfXPfe5zFWs+Zbe9qoY/IlYBlQ72frK57ZjZYPE3/MwS5fCbJcrhN0uUw2+WKIffLFEOv1mi/NPdHWDs2LGF9RkzZhTW999//4q1Sy+9tK6e+o0fP76wvnz58sL6kUce2dD8rXW85jdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJcvjNEqWIGLSZ9fT0RG9v76DNzyw1PT099Pb21vR7617zmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJcvjNEuXwmyXK4TdLlMNvliiH3yxRDr9Zohx+s0Q5/GaJqhp+SWMk3SdpnaTHJX01H361pOckPZbfTmt9u2bWLLVctGMncFlEPCppBPBbSSvy2vUR8Z+ta8/MWqVq+CNiI7Axv79d0jrgsFY3ZmattVuf+SV1Ax8BHs4HfVnSakmLJB1YYZrpknol9fb19TXUrJk1T83hlzQc+CkwIyJeAeYB44AJZFsGc8pNFxHzI6InInq6urqa0LKZNUNN4Ze0H1nwb42InwFExKaIeDMi3gIWABNb16aZNVste/sFLATWRcS3S4YfWjLa2cDa5rdnZq1Sy97+E4DzgTWSHsuHzQSmSpoABLAe+FJLOjSzlqhlb/8qoNzvgN/d/HbMbLD4G35miXL4zRLl8JslyuE3S5TDb5Yoh98sUQ6/WaIcfrNEOfxmiXL4zRLl8JslyuE3S5TDb5Yoh98sUYqIwZuZ1Af8sWTQKGDLoDWwezq1t07tC9xbvZrZ29iIqOn38gY1/LvMXOqNiJ62NVCgU3vr1L7AvdWrXb15s98sUQ6/WaLaHf75bZ5/kU7trVP7AvdWr7b01tbP/GbWPu1e85tZmzj8ZolqS/glTZb0e0lPSrqiHT1UImm9pDX5Zcd729zLIkmbJa0tGTZS0gpJf8j/lr1GYpt664jLthdcVr6ty67TLnc/6J/5JQ0BngBOATYAjwBTI+J3g9pIBZLWAz0R0fYvhEj6BLAD+GFEfDAf9u/Atoi4Ln/jPDAi/qVDersa2NHuy7bnV5M6tPSy8sBZwAW0cdkV9HUObVhu7VjzTwSejIinI+IN4MfAmW3oo+NFxAPAtgGDzwQW5/cXk714Bl2F3jpCRGyMiEfz+9uB/svKt3XZFfTVFu0I/2HAsyX/3kAbF0AZAdwj6beSpre7mTIOjoiNkL2YgNFt7megqpdtH0wDLivfMcuunsvdN1s7wl/u0l+ddLzxhIg4Dvg0cEm+eWu1qemy7YOlzGXlO0K9l7tvtnaEfwMwpuTfhwPPt6GPsiLi+fzvZuAOOu/S45v6r5Cc/93c5n7e1kmXbS93WXk6YNl10uXu2xH+R4Dxkt4naX9gCrCsDX3sQtKwfEcMkoYBp9J5lx5fBkzL708Dlraxl3folMu2V7qsPG1edp12ufu2fMMvP5RxAzAEWBQR1w56E2VIej/Z2h6yKxgvaWdvkm4DTiI75XMTMAu4E7gdOAJ4BvhsRAz6jrcKvZ1Etun69mXb+z9jD3Jvk4D/BtYAb+WDZ5J9vm7bsivoayptWG7+eq9ZovwNP7NEOfxmiXL4zRLl8JslyuE3S5TDb5Yoh98sUf8H9gaUpkXIqvAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "def print_number(num):\n",
    "    label = train_label[num].argmax(axis=0)\n",
    "    print(train_label[num])\n",
    "    image = train_data[num].reshape([28,28])\n",
    "    plt.title(\"Number: %i, label: %i\" %(num,label))\n",
    "    plt.imshow(image,cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()\n",
    "print_number(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\C5225062\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=input_shape))\n",
    "# 64 3x3 kernels\n",
    "model.add(Conv2D(64,activation='relu',kernel_size=(2,2)))\n",
    "# Reduce by taking the max of each 2x2 block\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "# Dropout to avoid overfitting\n",
    "model.add(Dropout(0.25))\n",
    "# Flatten the results to one dimension for passing into our final layer\n",
    "model.add(Flatten())\n",
    "# A hidden layer to learn with\n",
    "model.add(Dense(128,activation='relu'))\n",
    "# Another dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Final categorization from 0-9 with softmax\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double check the model description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 25, 25, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,189,642\n",
      "Trainable params: 1,189,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still doing multiple categorization, so categorical_crossentropy is still the right loss function to use. We'll use the Adam optimizer, although the example provided with Keras uses RMSProp. You might want to try both if you have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 - 150s - loss: 0.1513 - acc: 0.9540 - val_loss: 0.0486 - val_acc: 0.9839\n",
      "Epoch 2/10\n",
      "60000/60000 - 144s - loss: 0.0605 - acc: 0.9818 - val_loss: 0.0387 - val_acc: 0.9866\n",
      "Epoch 3/10\n",
      "60000/60000 - 127s - loss: 0.0440 - acc: 0.9862 - val_loss: 0.0290 - val_acc: 0.9902\n",
      "Epoch 4/10\n",
      "60000/60000 - 160s - loss: 0.0345 - acc: 0.9886 - val_loss: 0.0358 - val_acc: 0.9893\n",
      "Epoch 5/10\n",
      "60000/60000 - 134s - loss: 0.0276 - acc: 0.9908 - val_loss: 0.0305 - val_acc: 0.9912\n",
      "Epoch 6/10\n",
      "60000/60000 - 137s - loss: 0.0220 - acc: 0.9927 - val_loss: 0.0334 - val_acc: 0.9909\n",
      "Epoch 7/10\n",
      "60000/60000 - 133s - loss: 0.0201 - acc: 0.9936 - val_loss: 0.0298 - val_acc: 0.9907\n",
      "Epoch 8/10\n",
      "60000/60000 - 112s - loss: 0.0162 - acc: 0.9945 - val_loss: 0.0369 - val_acc: 0.9912\n",
      "Epoch 9/10\n",
      "60000/60000 - 108s - loss: 0.0152 - acc: 0.9949 - val_loss: 0.0320 - val_acc: 0.9914\n",
      "Epoch 10/10\n",
      "60000/60000 - 108s - loss: 0.0150 - acc: 0.9952 - val_loss: 0.0345 - val_acc: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "                    train_data,train_label,\n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                    validation_data=(test_data,test_label)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.034526\n",
      "Accuracy : 0.992000\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_data,test_label,verbose=0)\n",
    "print(\"Loss: %f\" %(score[0]))\n",
    "print(\"Accuracy : %f\" %(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
